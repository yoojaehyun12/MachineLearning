# 평가
## 분류 (Classfication) 성능 평가 지표
### 정확도(Accuracy)
- 예측 결과가 동일한 데이터 건 수 / 전체 예측 데이터 건수
    - 직관적으로 모델 예측 성능을 나타내는 평가 지표
         - 단, 이진분류의 경우 데이터 구성에 따라 모델의 성능을 왜곡 시켜버려서 정확도만으로는 모델 성능을 평가하지는 않음
         - 특히, 불균형한 레이블 값 분포에서 모델의 성능을 판단 할 때 적합한 지표가 아님.
- 정확도의 문제점
    - y = [0,0,0,0,0,0,0,0,0,1]
        - y의 데이터에서 굳이 모델을 만들지 않더라도 0으로 예측을 해도 정확도는 90%가 나오게 된다.
            - `그래서 정확도만으로는 모델의 성능을 판단할 수 는 없다.!`
 
### 오차행렬(Confusion Matrix)
- 이진 분류의 예측 오류가 얼마인지 + 어떤 유형의 예측 오류가 발생하고 있는지를 함께 나타내는 지표

    ![alt text](image-1.png)

    ![alt text](image-4.png)

### 정밀도(Precision)
- TP / (FP + TP)
- 예측을 Positive로 한 대상 중에 예측과 실제 값이 Positive 일치한 데이터 비율
    - 음성(Negative)을 양성(Positive)으로 잘못 예측 할 시 정밀도는 내려간다.
        - 정밀도가 내려 갈 시 업무상 큰 영향 발생되는 경우
             - 스팸 메일 등
             
### 재현율(Recall)
- TP / (FN + TP)
- 실제값을 Positive로 한 대상 중에 예측과 실제 값이 Positive 일치한 데이터 비율
    - 양성(Positive)을 음성(Negative)으로 잘못 예측 할 시 재현율이 내려간다.
        - 재현율이 내려 갈 시 업무상 큰 영향 발생되는 경우
            - 암 진단, 금융 사기 판별 등

### 정밀도(P)/재현율(R)의 트레이드 오프
- P가 올라가면 R이 내려가고, R이 올라가면 P가 내려간다.
    - 왜??
        - 정밀도와 재현율을 상호보완적인 지표이기 때문임
        - 어느 한쪽을 강제로 높이면 다른 하나의 수치는 떨어지기 쉽다.
            - 이를 `트레이드 오프(trade-off)` 라고 함.
- 분류 업무 할 때, 어쩔 수 없이 정밀도 혹은 재현율을 강조해야 될 경우 `분류의 결정 임계값(Threshold)`을 조정해 정밀도 혹은 재현율 수치 높일 수 있음. 

### 분류의 결정 임계값(Threshold)에 따른 Positive 예측 확률 변화
- Threshold 0.5 ⬇ -> 재현율 상승
- Threshold 0.5 ⬆ -> 정밀도 상승
### F1 Score
- 정확도를 나타내는 것이 아니라 재현율과 정밀도의 사이 벨런스가 잘 맞나? 확인하는 지표.

    ![alt text](image-3.png)
### ROC 곡선과 AUC