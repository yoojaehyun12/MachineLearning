{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e932ea",
   "metadata": {},
   "source": [
    "# UDF(User Defined Function)\n",
    "- 사용자 정의 함수\n",
    "- 스파크 데이터프레임에서 사용이 가능, SQL에서도 사용이 가능\n",
    "- 사용자가 만든 함수를 Worker의 Task에서 사용 가능하도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a50336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/29 11:20:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0.0.0.0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>udf</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7faa1b5672b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"udf\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e992a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [\n",
    "    ('찹쌀탕수육+짜장2', '2021-11-07 13:20:00', 22000, 'KRW'),\n",
    "    ('등심탕수육+크림새우+짜장면', '2021-10-24 11:19:00', 21500, 'KRW'),\n",
    "    ('월남 쌈 2인 세트', '2021-07-25 11:12:40', 42000, 'KRW'),\n",
    "    ('콩국수+열무비빔국수', '2021-07-10 08:20:00', 21250, 'KRW'),\n",
    "    ('장어소금+고추장구이', '2021-07-01 05:36:00', 68700, 'KRW'),\n",
    "    ('족발', '2020-08-19 19:04:00', 32000, 'KRW'),\n",
    "]\n",
    "\n",
    "schema = [\"name\", \"datetime\", \"price\", \"currency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c15faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, datetime: string, price: bigint, currency: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= spark.createDataFrame(data=transactions, schema=schema)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c3f3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-------------------+-----+--------+\n",
      "|                      name|           datetime|price|currency|\n",
      "+--------------------------+-------------------+-----+--------+\n",
      "|          찹쌀탕수육+짜장2|2021-11-07 13:20:00|22000|     KRW|\n",
      "|등심탕수육+크림새우+짜장면|2021-10-24 11:19:00|21500|     KRW|\n",
      "|          월남 쌈 2인 세트|2021-07-25 11:12:40|42000|     KRW|\n",
      "|       콩국수+열무비빔국수|2021-07-10 08:20:00|21250|     KRW|\n",
      "|       장어소금+고추장구이|2021-07-01 05:36:00|68700|     KRW|\n",
      "|                      족발|2020-08-19 19:04:00|32000|     KRW|\n",
      "+--------------------------+-------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3054384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6437f931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-------------------+-----+--------+\n",
      "|                      name|           datetime|price|currency|\n",
      "+--------------------------+-------------------+-----+--------+\n",
      "|          찹쌀탕수육+짜장2|2021-11-07 13:20:00|22000|     KRW|\n",
      "|등심탕수육+크림새우+짜장면|2021-10-24 11:19:00|21500|     KRW|\n",
      "|          월남 쌈 2인 세트|2021-07-25 11:12:40|42000|     KRW|\n",
      "|       콩국수+열무비빔국수|2021-07-10 08:20:00|21250|     KRW|\n",
      "|       장어소금+고추장구이|2021-07-01 05:36:00|68700|     KRW|\n",
      "|                      족발|2020-08-19 19:04:00|32000|     KRW|\n",
      "+--------------------------+-------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query= \"\"\"\n",
    "SELECT *\n",
    "FROM transactions\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d173b523",
   "metadata": {},
   "source": [
    "- UDF는 분산 병렬 처리 환경에서 사용할 수 있는 함수 만들 때 사용한다.(Worker에서 작동하는 함수)\n",
    "- 리턴 타입을 따로 지정하지 않으면 기본적으로 String을 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20104ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반드시 리턴 있어야함\n",
    "def squared(n):\n",
    "    return n * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "270759ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.squared(n)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# register(\"Worker에서 사용할 함수의 이름\", 마스터(클라이언트)에 정의된 함수의 이름, 리턴 타입)\n",
    "spark.udf.register(\"squared\", squared, LongType()) # 이름은 함수이름과 동일하게 하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e67fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|price|squared(price)|\n",
      "+-----+--------------+\n",
      "|22000|     484000000|\n",
      "|21500|     462250000|\n",
      "|42000|    1764000000|\n",
      "|21250|     451562500|\n",
      "|68700|    4719690000|\n",
      "|32000|    1024000000|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT price, squared(price)\n",
    "FROM transactions\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcdb4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_number(n):\n",
    "    units = [\"\", \"십\", \"백\", \"천\", \"만\"]\n",
    "    nums = '일이삼사오육칠팔구'\n",
    "    result = []\n",
    "    i = 0\n",
    "    while n > 0:\n",
    "        n, r = divmod(n, 10)\n",
    "        if r > 0:\n",
    "            result.append(nums[r-1]+units[i])\n",
    "        i += 1\n",
    "    return \"\".join(reversed(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8776491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이만구천구백구십'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_number(29990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a65f9577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.read_number(n)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"read_number\", read_number) # 리턴 타입을 지정하지 않으면 자동으로 문자열 타입으로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a865bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|price|read_number(price)|\n",
      "+-----+------------------+\n",
      "|22000|          이만이천|\n",
      "|21500|      이만일천오백|\n",
      "|42000|          사만이천|\n",
      "|21250|  이만일천이백오십|\n",
      "|68700|      육만팔천칠백|\n",
      "|32000|          삼만이천|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query=\"\"\"\n",
    "SELECT price, read_number(price)\n",
    "FROM transactions\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f595b07",
   "metadata": {},
   "source": [
    "# 데이터 프레임에서 사용할 udf 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd0dff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = \"/home/ubuntu/working/spark/data/titanic_train.csv\"\n",
    "\n",
    "# inferSchema : 컬럼 타입을 자동 추론\n",
    "titanic_sdf = spark.read.csv(filepath, inferSchema=True, header=True)\n",
    "titanic_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65ede836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "avg_age = titanic_sdf.select(F.avg(F.col('Age')))\n",
    "avg_age_row = avg_age.head()\n",
    "avg_age_value = avg_age.head()[0]\n",
    "\n",
    "# Spark DataFrame의 fillna()에 인자로 Dict를 입력하여 여러개의 컬럼들에 대해서 결측치 값을 입력할 수 있게 만들어줌.\n",
    "titanic_sdf_filled = titanic_sdf.fillna({'Age': avg_age_value,\n",
    "                                         'Cabin': 'C000',\n",
    "                                         'Embarked': 'S'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb3351e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이의 카테고리를 구하는 함수를 정의\n",
    "def get_category(age):\n",
    "    cat = ''\n",
    "\n",
    "    if age <= 5: cat = 'Baby'\n",
    "    elif age <= 12: cat = 'Child'\n",
    "    elif age <= 18: cat = 'Teenager'\n",
    "    elif age <= 25: cat = 'Student'\n",
    "    elif age <= 35: cat = 'Young Adult'\n",
    "    elif age <= 60: cat = 'Adult'\n",
    "    else : cat = 'Elderly'\n",
    "\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcd39d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임에서 API에서 udf 사용하기\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "udf_get_category = F.udf(lambda x : get_category(x), StringType())\n",
    "udf_get_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "243f154f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|              Age|AgeCategory|\n",
      "+-----------------+-----------+\n",
      "|             22.0|    Student|\n",
      "|             38.0|      Adult|\n",
      "|             26.0|Young Adult|\n",
      "|             35.0|Young Adult|\n",
      "|             35.0|Young Adult|\n",
      "|29.69911764705882|Young Adult|\n",
      "|             54.0|      Adult|\n",
      "|              2.0|       Baby|\n",
      "|             27.0|Young Adult|\n",
      "|             14.0|   Teenager|\n",
      "|              4.0|       Baby|\n",
      "|             58.0|      Adult|\n",
      "|             20.0|    Student|\n",
      "|             39.0|      Adult|\n",
      "|             14.0|   Teenager|\n",
      "|             55.0|      Adult|\n",
      "|              2.0|       Baby|\n",
      "|29.69911764705882|Young Adult|\n",
      "|             31.0|Young Adult|\n",
      "|29.69911764705882|Young Adult|\n",
      "+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_sdf_filled.withColumn(\"AgeCategory\", udf_get_category(F.col(\"Age\"))).select(\"Age\", \"AgeCategory\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a32ecf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceda08c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d221b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd764f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
